{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ac9a5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os \n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import pyarrow\n",
    "from datasets import load_dataset\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import time \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2444969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import pathlib \n",
    "\n",
    "file_paths = []\n",
    "for root, dirs, files in os.walk('/work/sunrui/pretrain_data'):\n",
    "    for filename in files:\n",
    "        filepath = os.path.join(root, filename)\n",
    "        if 'AIT17.0' in filepath:\n",
    "            continue\n",
    "        file_paths.append(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ab04d487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54f233de6a6d451ea77d5751da51d888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aaa236989a9401596d70a99721aff13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_1 = load_dataset(path = '/work/sunrui/pretrain_dataset/allen_2021_data',\n",
    "                         cache_dir = '/work/sunrui/huggingface')\n",
    "dataset_1 = dataset_1['train']\n",
    "data_dict = dataset_1.train_test_split(test_size = 0.05,\n",
    "                            shuffle = True,\n",
    "                            seed = 42)\n",
    "train_dataset = data_dict['train']\n",
    "test_dataset = data_dict['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1d9072db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1,2,3,4,5][1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "36bb7e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['count', 'gene_id_identifier', 'meta_info'],\n",
       "    num_rows: 1110752\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "100871d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mus_gene_0 = np.load('../gene_dict/mus_gene_0.npy',allow_pickle=True)\n",
    "mus_gene_1 = np.load('../gene_dict/mus_gene_1.npy',allow_pickle=True)\n",
    "\n",
    "mus_gene_0_id = np.load('../gene_dict/mus_gene_id_0.npy',allow_pickle=True)\n",
    "mus_gene_1_id = np.load('../gene_dict/mus_gene_id_1.npy',allow_pickle=True)\n",
    "\n",
    "dataset_gene = {}\n",
    "\n",
    "dataset_gene['mus_gene_0'] = list(mus_gene_0) \n",
    "dataset_gene['mus_gene_1'] = list(mus_gene_1) \n",
    "\n",
    "dataset_gene_ids = {}\n",
    "\n",
    "dataset_gene_ids['mus_gene_0'] = list(map(int,mus_gene_0_id))\n",
    "dataset_gene_ids['mus_gene_1'] = list(map(int,mus_gene_1_id))\n",
    "\n",
    "with open('../gene_dict/dataset_gene_info.json','w') as f:\n",
    "    json.dump(dataset_gene,f) \n",
    "\n",
    "with open('../gene_dict/dataset_gene_ids_info.json','w') as f:\n",
    "    json.dump(dataset_gene_ids,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ec863e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tokenizer_v1:\n",
    "    def __init__(self,gene_dict, dataset_gene, dataset_gene_ids):\n",
    "        self.gene_dict = gene_dict \n",
    "        self.dataset_gene = dataset_gene\n",
    "        self.dataset_gene_ids = dataset_gene_ids\n",
    "        #self.vocab_size = len(gene_dict)\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        \"\"\"返回词汇表大小\"\"\"\n",
    "        return len(self.gene_dict)\n",
    "\n",
    "\n",
    "    def add_token(self, token, index = None):\n",
    "        if index is None:\n",
    "            index = self.vocab_size\n",
    "        if token not in self.gene_dict:\n",
    "            if index not in self.gene_dict.values():\n",
    "                self.gene_dict[token] = index\n",
    "            else:\n",
    "                raise ValueError(\"index already exists\")\n",
    "        \n",
    "\n",
    "\n",
    "    def get_token_id(self, ids, gene_id_identifier):\n",
    "        # ids is a list\n",
    "        return self.dataset_gene_ids[gene_id_identifier][ids]\n",
    "    \n",
    "    def get_token_name(self, ids, gene_id_identifier):\n",
    "        return self.dataset_gene[gene_id_identifier][ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9f18bca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 指定json文件路径\n",
    "file_path = '../gene_dict/gene_dict.json'\n",
    "\n",
    "# 使用with语句打开文件，这样可以自动管理文件关闭\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    # 使用json.load()方法加载文件内容到字典\n",
    "    gene_dict = json.load(file)\n",
    "\n",
    "for key in dataset_gene:\n",
    "    dataset_gene[key] = np.array(dataset_gene[key])\n",
    "\n",
    "for key in dataset_gene_ids:\n",
    "    dataset_gene_ids[key] = np.array(dataset_gene_ids[key])\n",
    "\n",
    "tokenizer = tokenizer_v1(gene_dict= gene_dict,\n",
    "                         dataset_gene= dataset_gene,\n",
    "                         dataset_gene_ids= dataset_gene_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "02addd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gene_sample_1(count, max_num, rho = 0.1, pad_idx = -1):\n",
    "    # sample stratege 1:\n",
    "    # for cells with expressed gene num greater than max_num, random sample rho*max_num expressed gene and (1-rho)*max_num not expressed gene\n",
    "    # for cells with expressed gene num less than max_num, take all expressed gene and min(max_num - L_pos, L_pos) not expressed gene \n",
    "\n",
    "    # the probability of non-expressed gene being sampled is zero if based on counts\n",
    "    # provide a pseudo count, such that the non-expressed gene being sampled is rho  \n",
    "    # we have (N_neg*\\eta)/(umi_count + N*\\eta) = rho \n",
    "    # then we have \\eta = rho*umi_count / (N_neg - rho*N)\n",
    "    \n",
    "\n",
    "    # rho: the probability of zero-count genes being sampled, float 0-1 , default = 0.1\n",
    "    # return sample ids \n",
    "\n",
    "    if isinstance(count, torch.Tensor) :\n",
    "        count = count.numpy()\n",
    "    if isinstance(count, list):\n",
    "        count = np.array(count)\n",
    "    umi_count = count.sum()\n",
    "    N_neg = (count == 0).sum()\n",
    "    N = count.shape[0]\n",
    "    eta = max( (rho*umi_count) / (N_neg - rho*N) , 0) \n",
    "\n",
    "    sample_prob = (count + eta)/ (umi_count + N*eta)\n",
    "\n",
    "    sample_ids = np.random.choice(np.arange(N), size = max_num, replace=False, p = sample_prob)\n",
    "    return sample_ids\n",
    "\n",
    "\n",
    "class collater():\n",
    "    def __init__(self, tokenizer, max_expression, mask_ratio, max_num, rho, pad_idx = -1):\n",
    "        self.tokenizer = tokenizer \n",
    "        self.max_num = max_num \n",
    "        self.rho = rho \n",
    "        self.pad_idx = pad_idx\n",
    "        self.max_expression = max_expression\n",
    "        self.mask_ratio = mask_ratio\n",
    "        self.gene_cls_id = tokenizer.gene_dict['<cls>']\n",
    "        self.count_mask_id = max_expression + 1 \n",
    "        self.count_cls_id = max_expression + 2\n",
    "        #self.mask_id = tokenizer.gene_dict['<mask>']\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        batch_data = {} \n",
    "\n",
    "        batch_data['counts_0'] = []\n",
    "        batch_data['counts_1'] = []\n",
    "\n",
    "        batch_data['token_id_0'] = []\n",
    "        batch_data['token_id_1'] = [] \n",
    "\n",
    "        batch_data['mask_array'] = []\n",
    "\n",
    "        for sample in batch:\n",
    "            count = np.array(sample['count'])\n",
    "            gene_id_identifier = sample['gene_id_identifier'] \n",
    "\n",
    "            # perform down sample\n",
    "            sample_ids_0 = gene_sample_1(count, max_num = self.max_num, rho = self.rho, pad_idx= self.pad_idx)\n",
    "            sample_ids_1 = gene_sample_1(count, max_num = self.max_num, rho = self.rho, pad_idx= self.pad_idx)\n",
    "\n",
    "            # encode gene_id towards number id \n",
    "            token_id_0 = tokenizer.get_token_id(ids = sample_ids_0 ,gene_id_identifier= gene_id_identifier)\n",
    "            token_id_1 = tokenizer.get_token_id(ids = sample_ids_1 ,gene_id_identifier= gene_id_identifier) \n",
    "\n",
    "            # clip the counts \n",
    "\n",
    "            counts_0 = np.clip(count[sample_ids_0], a_min = 0, a_max = self.max_expression)\n",
    "            counts_1 = np.clip(count[sample_ids_1], a_min = 0, a_max = self.max_expression) \n",
    "\n",
    "            # mask the counts \n",
    "            mask_array = np.random.choice([True, False], size= self.max_num, p=[self.mask_ratio, 1-self.mask_ratio]) \n",
    "\n",
    "            # add the cls token in counts and token_id\n",
    "\n",
    "            #counts_0[mask_array] = self.count_mask_id \n",
    "            #counts_1[mask_array] = self.count_mask_id \n",
    "\n",
    "\n",
    "            token_id_0 = np.insert(token_id_0, 0, self.gene_cls_id)\n",
    "            token_id_1 = np.insert(token_id_1, 0, self.gene_cls_id) \n",
    "\n",
    "            counts_0 = np.insert(counts_0, 0, self.count_cls_id) \n",
    "            counts_1 = np.insert(counts_1, 0, self.count_cls_id) \n",
    "\n",
    "            mask_array = np.insert(mask_array, 0, False) \n",
    "\n",
    "            # add to batch_data \n",
    "            batch_data['counts_0'].append(counts_0)\n",
    "            batch_data['counts_1'].append(counts_1)\n",
    "            batch_data['token_id_0'].append(token_id_0)\n",
    "            batch_data['token_id_1'].append(token_id_1)\n",
    "            batch_data['mask_array'].append(mask_array)\n",
    "\n",
    "\n",
    "        batch_data['counts_0'] = torch.tensor(batch_data['counts_0'], dtype = torch.int)\n",
    "        batch_data['counts_1'] = torch.tensor(batch_data['counts_1'], dtype = torch.int) \n",
    "\n",
    "        batch_data['counts_0'] = batch_data['counts_0'].long()\n",
    "        batch_data['counts_1'] = batch_data['counts_1'].long()\n",
    "\n",
    "        batch_data['token_id_0'] = torch.tensor(batch_data['token_id_0'], dtype = torch.int)\n",
    "        batch_data['token_id_1'] = torch.tensor(batch_data['token_id_1'], dtype = torch.int)\n",
    "        batch_data['mask_array'] = torch.tensor(batch_data['mask_array'], dtype = torch.bool) \n",
    "\n",
    "        counts = torch.cat((batch_data['counts_0'], batch_data['counts_1']))\n",
    "        token_id = torch.cat((batch_data['token_id_0'], batch_data['token_id_1']))\n",
    "        mask_array = torch.cat((batch_data['mask_array'], batch_data['mask_array'])) \n",
    "        label = counts[mask_array]\n",
    "        counts[mask_array] = self.count_mask_id\n",
    "        \n",
    "        return (counts, token_id, mask_array, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c208747d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33525\n",
      "33525\n",
      "33524\n"
     ]
    }
   ],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "print(vocab_size)\n",
    "\n",
    "tokenizer.add_token(token = '<cls>')\n",
    "#tokenizer.gene_dict['<cls>'] = vocab_size\n",
    "print(tokenizer.vocab_size)\n",
    "print(tokenizer.gene_dict['<cls>'])\n",
    "collate_fn = collater(tokenizer= tokenizer, max_expression= 100, mask_ratio = 0.1, max_num = 6000,  rho = 0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ff1133",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm \n",
    "# 创建 DataLoader 实例\n",
    "batch_size = 32\n",
    "data_loader = DataLoader(combined_dataset, batch_size=batch_size, shuffle=True, collate_fn= collate_fn)\n",
    "\n",
    "token_count = {}\n",
    "for i in range(tokenizer.vocab_size + 1):\n",
    "    token_count[i] = 0 \n",
    "# 迭代数据\n",
    "for i,batch in tqdm(enumerate(data_loader),desc = 'Processing',total =len(data_loader)):\n",
    "    #counts = torch.cat((batch['counts_0'], batch['counts_1']))\n",
    "    #token_id = torch.cat((batch['token_id_0'], batch['token_id_1']))\n",
    "    #mask_array = torch.cat((batch['mask_array'], batch['mask_array'])) \n",
    "    counts, token_id, mask_array, label = batch\n",
    "    for ele in token_id.view(-1):\n",
    "        token_count[ele.item()] += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab097d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1.12.1",
   "language": "python",
   "name": "torch1.12.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
